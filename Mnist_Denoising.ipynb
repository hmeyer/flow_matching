{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmeyer/flow_matching/blob/main/Mnist_Denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smGbCO1TuWFq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijH1U7wEuf48",
        "outputId": "569a7832-0dd6-47cf-e794-52d7272401c4"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQVWR6EGuveY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzBXddvVr-1Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "from mediapy import show_image, show_images, show_video\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xHFtQfLvvKG"
      },
      "source": [
        "## Setup Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgjr7PT_vxbx",
        "outputId": "cea4f221-689a-4e98-e62a-ef40711a30b8"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RtQMZzXvI61"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y6dFWfkevG3N",
        "outputId": "ddfeebd8-60d5-4a18-c7c2-7678cc3e2717"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_dataset = Subset(test_dataset, range(1_000))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size * 2, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"train\")\n",
        "show_images(next(iter(train_loader))[0].squeeze(1))\n",
        "print(\"test\")\n",
        "show_images(next(iter(test_loader))[0].squeeze(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghHXDa4qw3Fi"
      },
      "source": [
        "## Denoising Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbByU5B-w6nJ"
      },
      "outputs": [],
      "source": [
        "class SinusoidalTimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = t[:, None] * embeddings[None, :]\n",
        "        if self.dim % 2 == 1: # zero pad if dim is odd for a final concat\n",
        "            pad_tensor = torch.zeros((embeddings.shape[0], 1), device=device)\n",
        "            embeddings = torch.cat((embeddings.sin(), embeddings.cos(), pad_tensor), dim=-1)\n",
        "        else:\n",
        "            embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, time_embedding_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(time_embedding_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, t_emb):\n",
        "        return self.mlp(t_emb)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim=None, num_groups=8): # Made time_emb_dim optional\n",
        "        super().__init__()\n",
        "        # Ensure num_groups is valid\n",
        "        effective_num_groups1 = min(num_groups, in_channels // 4 if in_channels >= 4 else 1) if in_channels > 0 else 1 # Use 1 if in_channels is 1, 2, 3\n",
        "        if in_channels > 0 and in_channels < effective_num_groups1 : # ensure num_channels >= num_groups\n",
        "            effective_num_groups1 = in_channels\n",
        "\n",
        "        effective_num_groups2 = min(num_groups, out_channels // 4 if out_channels >= 4 else 1) if out_channels > 0 else 1\n",
        "        if out_channels > 0 and out_channels < effective_num_groups2:\n",
        "            effective_num_groups2 = out_channels\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        # GroupNorm needs num_channels > 0 and num_groups <= num_channels\n",
        "        self.norm1 = nn.GroupNorm(num_groups=effective_num_groups2, num_channels=out_channels) if out_channels > 0 else nn.Identity()\n",
        "        self.act1 = nn.SiLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = nn.GroupNorm(num_groups=effective_num_groups2, num_channels=out_channels) if out_channels > 0 else nn.Identity()\n",
        "        self.act2 = nn.SiLU()\n",
        "\n",
        "        # Time projection only if time_emb_dim is provided and > 0\n",
        "        self.time_proj = nn.Linear(time_emb_dim, out_channels) if time_emb_dim is not None and time_emb_dim > 0 else None\n",
        "\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb=None): # Made t_emb optional\n",
        "        h = self.conv1(x)\n",
        "        h = self.norm1(h)\n",
        "\n",
        "        # Add time embedding if available and projected\n",
        "        if self.time_proj is not None and t_emb is not None:\n",
        "            time_cond = self.time_proj(t_emb)\n",
        "            # Ensure time_cond is not None (it shouldn't be if self.time_proj exists)\n",
        "            if time_cond is not None:\n",
        "                h = h + time_cond[:, :, None, None] # Expand dims for broadcasting\n",
        "\n",
        "        h = self.act1(h)\n",
        "\n",
        "        h = self.conv2(h)\n",
        "        h = self.norm2(h)\n",
        "        h = self.act2(h)\n",
        "\n",
        "        return h + self.res_conv(x) # Residual connection\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      s = x.shape\n",
        "      t = einops.rearrange(x, 'b c h w -> b (h w) c')\n",
        "      t = self.attn(t, t, t)[0]\n",
        "      return x + einops.rearrange(t, 'b (h w) c -> b c h w', c=s[1], h=s[2])\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    # Modified to make time_emb_dim optional for ConvBlock\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim=None, num_groups_norm=8):\n",
        "        super().__init__()\n",
        "        self.conv = ConvBlock(in_channels, out_channels, time_emb_dim, num_groups_norm)\n",
        "        # MaxPool before Conv is common in some UNets, after in others. Let's keep it before like typical ResNets.\n",
        "        # Or, is the intention from the original UNet structure Pool -> Conv? Let's assume Pool -> Conv\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x, t_emb=None): # Made t_emb optional\n",
        "        x = self.pool(x)\n",
        "        x = self.conv(x, t_emb)\n",
        "        return x\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels_prev_up, channels_from_skip, out_channels_conv, time_emb_dim, num_groups_norm):\n",
        "        super().__init__()\n",
        "        # Upsample the features from the previous (lower) level\n",
        "        self.up_conv_transpose = nn.ConvTranspose2d(in_channels_prev_up, in_channels_prev_up // 2, kernel_size=2, stride=2)\n",
        "        # The ConvBlock will take the concatenated (upsampled features + skip features)\n",
        "        # channels_from_skip are the channels from the encoder path\n",
        "        # in_channels_prev_up // 2 are the channels after upsampling\n",
        "        self.conv = ConvBlock( (in_channels_prev_up // 2) + channels_from_skip, out_channels_conv, time_emb_dim, num_groups_norm)\n",
        "\n",
        "    def forward(self, x_prev_up, x_skip, t_emb):\n",
        "        x_upsampled = self.up_conv_transpose(x_prev_up)\n",
        "        # Ensure spatial dimensions match for concatenation (can be an issue with odd dimensions if not handled by padding in ConvTranspose2d or pool)\n",
        "        # For standard MNIST 28->14->7, then 7->14->28, this should be fine.\n",
        "        # If there's a mismatch:\n",
        "        # diffY = x_skip.size()[2] - x_upsampled.size()[2]\n",
        "        # diffX = x_skip.size()[3] - x_upsampled.size()[3]\n",
        "        # x_upsampled = F.pad(x_upsampled, [diffX // 2, diffX - diffX // 2,\n",
        "        #                                   diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x_cat = torch.cat([x_upsampled, x_skip], dim=1)\n",
        "        x = self.conv(x_cat, t_emb)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetMNIST(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=1,\n",
        "                 out_channels=1,\n",
        "                 base_channels=32, # C\n",
        "                 time_embedding_dim=128, # D_t\n",
        "                 time_mlp_hidden_dim=512,\n",
        "                 num_groups_norm=8,\n",
        "                 use_attention_bottleneck=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if base_channels <= 0:\n",
        "            raise ValueError(\"base_channels must be positive.\")\n",
        "        if time_embedding_dim <= 0 and time_mlp_hidden_dim > 0 : # If no time embedding, MLP for it is not needed.\n",
        "             time_embedding_dim = 0 # ensure consistency\n",
        "             time_mlp_hidden_dim = 0\n",
        "\n",
        "\n",
        "        self.time_embedding = SinusoidalTimeEmbedding(time_embedding_dim) if time_embedding_dim > 0 else nn.Identity()\n",
        "        self.time_mlp = MLP(time_embedding_dim, time_mlp_hidden_dim, time_embedding_dim) if time_embedding_dim > 0 else nn.Identity()\n",
        "\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv_in = ConvBlock(in_channels, base_channels, time_embedding_dim, num_groups_norm) # HxW -> HxW (C)\n",
        "\n",
        "        # Encoder\n",
        "        self.down1 = DownBlock(base_channels, base_channels * 2, time_embedding_dim, num_groups_norm)     # HxW (C) -> H/2 x W/2 (2C)\n",
        "        self.down2 = DownBlock(base_channels * 2, base_channels * 4, time_embedding_dim, num_groups_norm) # H/2 x W/2 (2C) -> H/4 x W/4 (4C)\n",
        "\n",
        "        # Bottleneck\n",
        "        # It operates on the output of down2 (4C channels)\n",
        "        self.bottleneck_conv1 = ConvBlock(base_channels * 4, base_channels * 8, time_embedding_dim, num_groups_norm) # H/4 x W/4 (4C) -> H/4 x W/4 (8C)\n",
        "        if use_attention_bottleneck:\n",
        "            self.attention = SelfAttentionBlock(base_channels * 8, num_heads=4) # Operates on 8C channels\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "        self.bottleneck_conv2 = ConvBlock(base_channels * 8, base_channels * 4, time_embedding_dim, num_groups_norm) # H/4 x W/4 (8C) -> H/4 x W/4 (4C)\n",
        "\n",
        "        # Decoder\n",
        "        # Takes output from bottleneck (4C) and skip from down1 (2C)\n",
        "        self.up1 = UpBlock(in_channels_prev_up=base_channels * 4, channels_from_skip=base_channels * 2, out_channels_conv=base_channels * 2, time_emb_dim=time_embedding_dim, num_groups_norm=num_groups_norm) # H/4 x W/4 (4C from bottleneck) + H/2 x W/2 (2C skip from s2) -> H/2 x W/2 (2C)\n",
        "\n",
        "        # Takes output from up1 (2C) and skip from conv_in (C)\n",
        "        self.up2 = UpBlock(in_channels_prev_up=base_channels * 2, channels_from_skip=base_channels, out_channels_conv=base_channels, time_emb_dim=time_embedding_dim, num_groups_norm=num_groups_norm)    # H/2 x W/2 (2C from up1) + H x W (C skip from s1) -> H x W (C)\n",
        "\n",
        "        # Output\n",
        "        self.conv_out = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x: (batch_size, in_channels, H, W)\n",
        "        # t: (batch_size,) scalar time values\n",
        "\n",
        "        if isinstance(self.time_embedding, nn.Identity):\n",
        "            t_emb = None # No time embedding used\n",
        "        else:\n",
        "            t_emb_sin = self.time_embedding(t)\n",
        "            t_emb = self.time_mlp(t_emb_sin)\n",
        "\n",
        "        # Encoder\n",
        "        s1 = self.conv_in(x, t_emb)       # (B, C, H, W)\n",
        "        s2 = self.down1(s1, t_emb)      # (B, 2C, H/2, W/2)\n",
        "        s3 = self.down2(s2, t_emb)      # (B, 4C, H/4, W/4)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck_conv1(s3, t_emb)\n",
        "        b = self.attention(b)\n",
        "        b = self.bottleneck_conv2(b, t_emb) # Output channels 4C\n",
        "\n",
        "        # Decoder\n",
        "        u1 = self.up1(b, s2, t_emb)         # Input to up1: b (4C), skip s2 (2C) -> Output 2C\n",
        "        u2 = self.up2(u1, s1, t_emb)        # Input to up2: u1 (2C), skip s1 (C)  -> Output C\n",
        "\n",
        "        out = self.conv_out(u2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPYLgk6PxCJT"
      },
      "source": [
        "## Denoising Loss and sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W60lpIvxEj5"
      },
      "outputs": [],
      "source": [
        "def denoising_loss(model, batch):\n",
        "    x0 = batch[0]\n",
        "    batch_size = x0.shape[0]\n",
        "    t = torch.rand((batch_size,), device=x0.device)\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_t = (1 - t[..., None, None, None]) * x0 + t[..., None, None, None] * noise\n",
        "    # Forward pass\n",
        "    x_hat = model(x_t, t)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = F.mse_loss(x_hat, x0)\n",
        "\n",
        "    return {\"loss\": loss}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_batch(model, noise, num_steps=10):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    x = torch.zeros_like(noise)\n",
        "    for i in range(num_steps):\n",
        "        t = torch.tensor([(num_steps - i) / num_steps] * x.shape[0], device=device)\n",
        "        x = (1 - t[..., None, None, None]) * x + t[..., None, None, None] * noise\n",
        "        x = model(x, t)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsa511LrxTCb"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKijwQ5sxUbI"
      },
      "outputs": [],
      "source": [
        "class TrainLossLogger:\n",
        "  def __init__(self, log_every_sec=10.0) -> None:\n",
        "    self.running = defaultdict(list)\n",
        "    self.log_every_sec = log_every_sec\n",
        "    self.last_log = time.time()\n",
        "\n",
        "  def log(self, data) -> None:\n",
        "    for k, v in data.items():\n",
        "      self.running[k].append(v)\n",
        "    if self.last_log + self.log_every_sec < time.time():\n",
        "      self.last_log = time.time()\n",
        "      print(\"mean train:\", {k: {float(np.array(v).mean())} for k, v in self.running.items()})\n",
        "      self.running = defaultdict(list)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, *, model, train_loader, test_loader, loss_fn, extra_eval_fn=None, num_epochs=5, batch_size=32):\n",
        "        self.model = model\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.loss_fn = loss_fn\n",
        "        self.extra_eval_fn = extra_eval_fn\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "    def train(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "        loss_logger = TrainLossLogger()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"{epoch=} test {self.eval()}\")\n",
        "            if self.extra_eval_fn:\n",
        "                self.extra_eval_fn(self.model)\n",
        "            self.model.train()\n",
        "            for i, batch in tqdm(enumerate(self.train_loader), desc=f\"Epoch {epoch + 1}/{self.num_epochs}\", total=len(self.train_loader), unit=\" batches\"):\n",
        "                batch = [x.to(self.device) for x in batch]\n",
        "                optimizer.zero_grad()\n",
        "                loss_dict = self.loss_fn(self.model, batch)\n",
        "                loss = loss_dict[\"loss\"]\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_logger.log({k: v.item() for k, v in loss_dict.items()})\n",
        "        print(f\"{epoch=} test {self.eval()}\")\n",
        "        if self.extra_eval_fn:\n",
        "            self.extra_eval_fn(self.model)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval(self) -> float:\n",
        "        torch.random.manual_seed(17)\n",
        "        self.model.eval()\n",
        "        total = defaultdict(list)\n",
        "        for batch in self.test_loader:\n",
        "            batch = [x.to(self.device) for x in batch]\n",
        "            loss = self.loss_fn(self.model, batch)\n",
        "            for k, v in loss.items():\n",
        "              total[k].append(v.item())\n",
        "        torch.random.seed()\n",
        "        return {k: float(np.array(v).mean()) for k, v in total.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWwKeoA5yx2I"
      },
      "source": [
        "## Train the Denoising model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845,
          "referenced_widgets": [
            "f1686c427494480a931b4e41b94d524a",
            "6467c08d45d84d568f007151f2ab24da",
            "5e2f97f2dbb04d859fd73baf3ce23060",
            "c605a962f5d24a40a1f8fd2a1e062d44",
            "5f6b6c85262f4a60ad2a07f2e40288e5",
            "996979d4c74f41e2b160c4e5e046f6b1",
            "af452c72936c4bdcb2a2fabd2ac71956",
            "8a5cc341a1a04069bd2f7189a9c6c051",
            "f65ba53674ec4d249a975ff68e845e70",
            "4c52999f864749a296b2e4251383dad3",
            "bb917bb8ba754c78a2f5887ef4da8a21",
            "54f146c382ce498dbc5b1eb60fa206a5",
            "16d7053904894e1ca786c6c30e0163c0",
            "576e04c4cf3c4b8884bea885db23a9bb",
            "38b7e1175ab640569c5b6a58d0c93c35",
            "322ec55c6fc246ec88e845b81bccfd10",
            "c4749bbe9f0b488a9e39524d387c8b28",
            "5e2fbefde5de4a12b4d4d03cce59a2be",
            "29c48775d4434233bf1f6ef29cfe3de0",
            "3e69e17fbb244b17a96b08559d5a58cf",
            "0020d4ef59b84233b5ed046cc5fc86ca",
            "648439d1bc724047be66b1d779976ca7",
            "1e8ce251015345f090c93576319d75da",
            "debf644b9e5a49b18cd92ef9840c3ae9",
            "4efd0f37a2644ff3b652f25551687f02",
            "f5882abf1573462fbb2b730c6baaa0c4",
            "2efc8320f0f94d7e8bcc6a91735bc47c",
            "eb781db9b7a54da0bd9961fa87b912c7",
            "117a58b45ebf4eda9e209def72866875",
            "2c0cef09957c409cad08128b7088f32c",
            "53ee76a2b73b4b51bd84324754bb4557",
            "e40bc53308f2449884c21338e443cc3c",
            "d9bc7db2cb8d487bb8afd07b944b4a23",
            "799744118ead432695288afef2f53687",
            "d79382ec56244c8d9aab9f5111dae903",
            "134aaeb602f14048ae9e2fc308ca28e7",
            "66062f321e014c27991f547c258200b8",
            "808c760d956f4a84bdcf6bf2022929ba",
            "8b4014481d974f47a6a382a60b2d943a",
            "f414dea96e244c2cb3b75c0f9c788ca2",
            "45292cf5482e4caba718458c3c95a222",
            "f0381fff458344e6a4873a57231c3994",
            "6e8baade544645cd8253ae0648965151",
            "5e78d58eabb24f1593d34ece9d486c05",
            "817bfa79d85742008169c96ed6af7f1d",
            "3a8f60841f714ee1bb68d8e5a606ef76",
            "2bb9ee2fb56a43e5acc2360dfbecef45",
            "eb577262a349476082ac7cf1765236de",
            "c956766fa7a345138bb960955b580fd6",
            "4929a3dd3e4d4974827a75bcef1dc8c6",
            "9370720515ae479185fe1d7d8f8f2c77",
            "7e39b565e14b465081df6e089d5fb3f6",
            "2efbecece4334e608c1af3ee2bfdc64c",
            "f9eb2bc91b084871aaac0cf97861d235",
            "514e9a7a9b064609b5bb759370595e4a"
          ]
        },
        "id": "x3FzfEzSy5K-",
        "outputId": "3401366d-33c3-4046-c1c3-05ee58d2924e"
      },
      "outputs": [],
      "source": [
        "fm_model = UNetMNIST()\n",
        "fm_model.to(device)\n",
        "\n",
        "def sample_and_viz(fm_model):\n",
        "  torch.random.manual_seed(1)\n",
        "  imgs = sample_batch(fm_model, noise=torch.randn((20, 1, 28, 28), device=device), num_steps=10).squeeze(1).cpu()\n",
        "  show_images(imgs)\n",
        "  _ = torch.random.seed()\n",
        "\n",
        "\n",
        "trainer = Trainer(model=fm_model,\n",
        "                  train_loader=train_loader,\n",
        "                  test_loader=test_loader,\n",
        "                  loss_fn=denoising_loss,\n",
        "                  extra_eval_fn=sample_and_viz,\n",
        "                  num_epochs=5)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlEtr-b_gmUk"
      },
      "source": [
        "## More Sampling fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "f36a2fefe0564177bee0622027334d92",
            "a0cf4f0bfca1415780716f0c7b21f0bf",
            "15a5b56729264e5883be5959d86928e4",
            "1cd2e329ad5944d68406e1900739e03d",
            "a1bfe4c756364556a03356149ea1e570",
            "376fde0ad1324c9d882fb5344effb9cc",
            "b73a6aca9b58407c9fb5e0bb9f756a70",
            "50d8a641dc034088b221192b0de48a57",
            "5ee239d3846b4753b42c7f887adf7b96",
            "fcaa10aa01aa4973a85e3b4dacd0b325",
            "6672bd1b997c43b2a57df06b22d58157",
            "3bfda2bbcaef41ec8bea3e618eedc5de",
            "a5736137f5bb49d8b1c7280983fc6b18",
            "354bf947a5be4161a6a3da71ea48418a",
            "46d1e4fc3db04256baac2856c807a292",
            "e25d32250eff4ccbb81b305c428c32a0",
            "84f055844f49498ead63bf8ce5efb835",
            "a56c6e0f1c0848eab2c835b5124036c0",
            "89473434fdd548d28c09873c09f5a19d",
            "2a91e6ad422b485cb28d539b9173b1dc",
            "db8fd3d86fcd44b18463d0d164aecaa4",
            "d7e36baa187a4e8da8b68f660284e7f1",
            "304d1e08e3b745ec9d1e1591dd505706",
            "b99593723874478a9cc5cd16249a4ddd",
            "034316a8f5ae41f0a644b2d85d3a4823",
            "dc2b3ce73682427583c0920cb0c36b72",
            "adac1fc168a74cd3b302e43f5d5201ff",
            "209002764dc541cd9d74f0ba8679c47b",
            "cf7eddc3c6594a0abc43b045d91c35ef",
            "36f94972ec3342b8b611b15ec3cecbef",
            "46c27fd0860b4794bdd30e26d17f1dd6",
            "4c55b5e2e3cc44109dc7627a88e88caa",
            "9ad9f7341dd548d4a0bda9153f0252ce",
            "60a1ca2651e4443f847763060533ad81",
            "36aeec8c7a8a40c0b20b156c8e432fd5",
            "3e6fed6a40a54793a87814ba013eae9f",
            "0196cae99886483eaa4fda1dcfe3285b",
            "35d0669a6a314fdb958458814f93627b",
            "c4b52ed458624b9eb1dcfa4452d26d2f",
            "d0e5f73c096a436992b14a69211a67d0",
            "d59d7f9f64384b26a2f58eb7df80f9fe",
            "0f73af8105f843568eefbc0be130fa82",
            "df28a140f7cb49c7b76fb9642f27822b",
            "86a1e2ce45d54d83904e52ab1fede1be",
            "7cfe712287e545cf99721ea2a3cd356a",
            "bca617eaa6df41f6bfaba77d9de165e6",
            "7ca074ed50484191a16aeb59a1865a53",
            "f21a301982c44254bc842b93483d06c1",
            "b8bc296971e0478583ff833a6f373e0a",
            "9998f7d5b9ef4fc88de814e1dff85621",
            "4a5c410fe7d84b9c83973aab3ac62aec",
            "f220bcb7f913449eadaf8504c3917d6d",
            "a561e7a892d74b2198353df21af2bf43",
            "5deb8726b0ed4dd1b2e80ae6cc6f9e55",
            "a57a83498c1243258cc6974863c129d9",
            "0628db0d5b844b69b1dad52d3576043e",
            "bbb6d451f6904eda883e0801af0963aa",
            "12312e80146c4f99a4f2756e9bf7fe81",
            "b1259754330d4f7eb235c14868bc8136",
            "ceb15b7ec13b4a7a994babc15453cf23",
            "d616558bf3a9441eaa5cd10ba6cfe142",
            "c304e822c6b146ef901af23d345ae402",
            "487cb5d8b8ca4200bdf52a3976633766",
            "d73479125c2b42b09c894b18e97798a1",
            "d89f6a40329045498be92804fc1afc28",
            "66fc2103cae7408391c7e384ac82d45f"
          ]
        },
        "id": "5U6mqAOKgoPm",
        "outputId": "1acf5c71-ac07-43e2-e186-9b4bce22d07b"
      },
      "outputs": [],
      "source": [
        "def lerp(a, b, t):\n",
        "  x = a * (1 -t) + b * t\n",
        "  return x\n",
        "\n",
        "def animate(fm_model, points=10, samples_between=100, cols=5, rows=5):\n",
        "  torch.random.manual_seed(1)\n",
        "  noises = [torch.randn((cols * rows, 1, 28, 28)) for _ in range(points)]\n",
        "  noises.append(noises[0])  # loop\n",
        "  images = []\n",
        "  for a, b in tqdm(zip(noises[:-1], noises[1:]), unit=\" points\"):\n",
        "    for t in tqdm(range(samples_between), unit=\" samples\"):\n",
        "      n = lerp(a, b, t / samples_between)\n",
        "      images.append(sample_batch(fm_model, noise=n.to(device), num_steps=10).squeeze(1).cpu())\n",
        "  i = torch.stack(images, axis=0).clip(0, 1)\n",
        "  i = einops.rearrange(i, \"n (gw gh) w h -> n (gw w) (gh h)\", gw=cols, gh=rows)\n",
        "  show_video(np.array(i[...] * 255).astype(np.uint8), fps=20, codec='gif')\n",
        "\n",
        "animate(fm_model, points=5, samples_between=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_lKJysDUMDv"
      },
      "source": [
        "## Simple Guidance (just brightness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "9d33835579cf4903900a56493e20ccf8",
            "ca9a085a59194b3f90c7325d94ec8e95",
            "02ef932172a741bf917f14667828bc73",
            "38b4ac2e1464413293a5e971fedee9d9",
            "3b948825f9f24e8f884f8adbc40c923d",
            "33dac982e6a7479289f07f57501a7a13",
            "975f5f2b19ce4aaab5c6fa12abaad9a1",
            "3d929657c49f4848af75326540aeebc4",
            "3af96c7ddaa24a16858695822338055e",
            "0924ca185e48443fb8944c7b3f544caf",
            "e95b64fa6c9c42ad9f968943eb8cccea"
          ]
        },
        "id": "AiIRJEwZUOdF",
        "outputId": "e486ed39-2251-442b-e382-807b372f0a1f"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_with_guidance(model, noise, guidance_fn, guidance_scale, num_steps=10):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    x = torch.zeros_like(noise)\n",
        "    for i in range(num_steps):\n",
        "      t = torch.tensor([(num_steps - i) / num_steps] * x.shape[0], device=device)\n",
        "      x = (1 - t[..., None, None, None]) * x + t[..., None, None, None] * noise\n",
        "      x = model(x, t)\n",
        "\n",
        "      x_for_guidance = x.detach().clone().requires_grad_(True)\n",
        "      with torch.enable_grad():\n",
        "        scores = guidance_fn(x_for_guidance)\n",
        "        grad = torch.autograd.grad(scores, x_for_guidance)[0]\n",
        "\n",
        "      x += guidance_scale * grad\n",
        "    return x\n",
        "\n",
        "\n",
        "def intensity_guidance(num_examples=10, steps=30):\n",
        "  def intensity(x):\n",
        "    return x.mean()\n",
        "\n",
        "  noise = torch.randn((num_examples, 1, 28, 28), device=device)\n",
        "\n",
        "  images = []\n",
        "  for s in tqdm(range(steps), total=steps):\n",
        "    scale = torch.tensor(s / (steps - 1)) * 2 - 1\n",
        "    scale *= 1e3\n",
        "\n",
        "    images.append(sample_with_guidance(fm_model, noise=noise, num_steps=100, guidance_fn=intensity, guidance_scale=scale).squeeze(1).cpu())\n",
        "  i = torch.stack(images, axis=0).clip(0, 1)\n",
        "  i = einops.rearrange(i, \"r c w h -> (c w) (r h)\")\n",
        "  show_image(i)\n",
        "\n",
        "\n",
        "intensity_guidance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWjsEuGJpak"
      },
      "source": [
        "## Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTw5vL-jJxca"
      },
      "outputs": [],
      "source": [
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=1,         # MNIST is grayscale\n",
        "                 num_classes=10,        # MNIST has 10 digits\n",
        "                 base_channels=32,      # Starting number of channels\n",
        "                 num_groups_norm=8,\n",
        "                 use_attention_bottleneck=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if base_channels <= 0:\n",
        "            raise ValueError(\"base_channels must be positive.\")\n",
        "\n",
        "        # --- Feature Extractor using UNet Encoder/Bottleneck Structure ---\n",
        "        # Time embedding components are not needed for standard classification\n",
        "        # We pass time_emb_dim=None to the blocks\n",
        "\n",
        "        # Initial convolution (maintains size: 28x28 -> 28x28)\n",
        "        # Input: 1x28x28\n",
        "        self.conv_in = ConvBlock(in_channels, base_channels, time_emb_dim=None, num_groups=num_groups_norm)\n",
        "        # Output: C x 28 x 28\n",
        "\n",
        "        # Encoder Path\n",
        "        # Downsample 1 (28x28 -> 14x14)\n",
        "        self.down1 = DownBlock(base_channels, base_channels * 2, time_emb_dim=None, num_groups_norm=num_groups_norm)\n",
        "        # Output: 2C x 14 x 14\n",
        "        # Downsample 2 (14x14 -> 7x7)\n",
        "        self.down2 = DownBlock(base_channels * 2, base_channels * 4, time_emb_dim=None, num_groups_norm=num_groups_norm)\n",
        "        # Output: 4C x 7 x 7\n",
        "\n",
        "        # Bottleneck (operates at 7x7 resolution)\n",
        "        # Increase channels\n",
        "        self.bottleneck_conv1 = ConvBlock(base_channels * 4, base_channels * 8, time_emb_dim=None, num_groups=num_groups_norm)\n",
        "        # Output: 8C x 7 x 7\n",
        "\n",
        "        # Optional Self-Attention\n",
        "        if use_attention_bottleneck:\n",
        "            self.attention = SelfAttentionBlock(base_channels * 8, num_heads=4)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "        # Output: 8C x 7 x 7\n",
        "\n",
        "        # Decrease channels back (ready for potential decoder, but we stop here)\n",
        "        # We will use the output of attention (or bottleneck_conv1 if no attention) for classification\n",
        "        # Let's use the output AFTER attention (8C channels) for max feature richness at bottleneck\n",
        "        bottleneck_out_channels = base_channels * 8\n",
        "\n",
        "        # --- Classification Head ---\n",
        "        # Global Average Pooling reduces spatial dimensions (7x7) to 1x1\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Output: 8C x 1 x 1\n",
        "\n",
        "        # Final Linear layer to map features to class scores\n",
        "        self.classifier_head = nn.Linear(bottleneck_out_channels, num_classes)\n",
        "        # Output: num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, in_channels, H, W) = (B, 1, 28, 28)\n",
        "\n",
        "        # Feature Extraction (Encoder + Bottleneck)\n",
        "        # Time embedding `t_emb` is None throughout\n",
        "        h = self.conv_in(x, t_emb=None)      # (B, C, 28, 28)\n",
        "        h = self.down1(h, t_emb=None)        # (B, 2C, 14, 14)\n",
        "        h = self.down2(h, t_emb=None)        # (B, 4C, 7, 7)\n",
        "        h = self.bottleneck_conv1(h, t_emb=None) # (B, 8C, 7, 7)\n",
        "        h = self.attention(h)              # (B, 8C, 7, 7)\n",
        "\n",
        "        # Classification Head\n",
        "        h = self.global_avg_pool(h)         # (B, 8C, 1, 1)\n",
        "        h = torch.flatten(h, 1)             # (B, 8C)\n",
        "        out_logits = self.classifier_head(h) # (B, num_classes)\n",
        "\n",
        "        return out_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPjvN0ZM2pZ0"
      },
      "source": [
        "## CE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIeU-crm2r7K"
      },
      "outputs": [],
      "source": [
        "def ce_loss(model, batch):\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    # Forward pass\n",
        "    y_hat = model(x)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = nn.CrossEntropyLoss()(y_hat, y)\n",
        "\n",
        "    predicted_classes = torch.argmax(y_hat, dim=1) # Shape: [batch_size]\n",
        "    accuracy = (predicted_classes == y).to(torch.float32).mean().detach().cpu()\n",
        "    return dict(loss=loss, accuracy=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPtMGI5qLC6e"
      },
      "source": [
        "## Train the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "c15b3d7bef364b90a090625177d73cc9",
            "488c1d526e7542548f20a576002a88ac",
            "635b8d9e279c4cd286ba59d94fbd20af",
            "dafc2639af1a41cdb17b94c4d814cc74",
            "d7c310e8dfca4d2d81c97ac40830c4e8",
            "3e61840f164b47a29b2ee860e0bf196b",
            "37eb4e011cd94abca61117e77b4aa276",
            "28412e8a7c444aa5b5e35bd6a16d540c",
            "78c7b6dd1d334d958f2ecd06c4309e11",
            "33543131751d424cabffb4e7d3958361",
            "7861b9980b0a46b198bdd16444699fb1"
          ]
        },
        "id": "ehn_OXC6LEpS",
        "outputId": "9c6ec2b0-e382-4488-bee1-f94ec0697f80"
      },
      "outputs": [],
      "source": [
        "classifier = MNISTClassifier()\n",
        "classifier.to(device)\n",
        "\n",
        "\n",
        "trainer = Trainer(model=classifier,\n",
        "                  train_loader=train_loader,\n",
        "                  test_loader=test_loader,\n",
        "                  loss_fn=ce_loss,\n",
        "                  extra_eval_fn=None,\n",
        "                  num_epochs=1)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACMjun9AbCil"
      },
      "source": [
        "\n",
        "## Classifier Guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "31adfb5c705a4fc488ba3bb8e36a880c",
            "0abb29085daa40978a53e093b9b89d08",
            "853994a5a9924928944c281f1f2bedfc",
            "f7fc94adfbbd413b8092fdc2ff93e884",
            "ec95088a2ace4eba9908d12a87d5f5a1",
            "1f2ac20ab7674eb3b7ba4bc5e0512be0",
            "d44ec9e36e37438d89628f56e3de20f5",
            "4b106847c1794c9cb8804615ffccb113",
            "0b1649f88dba4346847cd20c6e3b8ff9",
            "c1038f728f654880b5426c0d7b38ba10",
            "b9c9683f40ab45d4aada58209c577013"
          ]
        },
        "id": "7d2mBkYNbCIo",
        "outputId": "fbe14d6c-afb0-4bea-9e71-67c63ee7f260"
      },
      "outputs": [],
      "source": [
        "def class_scorer(classifier, x, class_weights):\n",
        "  logits = classifier(x)\n",
        "  log_probs = F.log_softmax(logits, dim=-1)\n",
        "  return (log_probs * class_weights[None, :]).sum(dim=-1).mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def number_guidance(num_examples=10):\n",
        "  noise = torch.randn((num_examples, 1, 28, 28), device=device)\n",
        "\n",
        "  images = []\n",
        "  for n in tqdm(range(10)):\n",
        "    one_hot = F.one_hot(torch.tensor(n), 10).to(device)\n",
        "    def scorer(x):\n",
        "      return class_scorer(classifier, x, one_hot)\n",
        "    images.append(sample_with_guidance(fm_model, noise=noise, num_steps=100, guidance_fn=scorer, guidance_scale=20).squeeze(1).cpu())\n",
        "  i = torch.stack(images, axis=0).clip(0, 1)\n",
        "  i = einops.rearrange(i, \"r c w h -> (c w) (r h)\")\n",
        "  show_image(i)\n",
        "\n",
        "number_guidance()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "smGbCO1TuWFq",
        "TQVWR6EGuveY",
        "_xHFtQfLvvKG",
        "-RtQMZzXvI61",
        "ghHXDa4qw3Fi",
        "wPYLgk6PxCJT",
        "gsa511LrxTCb",
        "wCWjsEuGJpak",
        "EPjvN0ZM2pZ0"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
